{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /jet/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: torchtext in /jet/lib/python3.6/site-packages (0.3.1)\n",
      "Requirement already satisfied: tqdm in /jet/lib/python3.6/site-packages (4.26.0)\n",
      "Requirement already satisfied: requests in /jet/lib/python3.6/site-packages (from torchtext) (2.19.1)\n",
      "Requirement already satisfied: numpy in /jet/lib/python3.6/site-packages/numpy-1.13.3-py3.6-linux-x86_64.egg (from torchtext) (1.13.3)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /jet/lib/python3.6/site-packages (from requests->torchtext) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.21.1 in /jet/lib/python3.6/site-packages (from requests->torchtext) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /jet/lib/python3.6/site-packages (from requests->torchtext) (2018.8.24)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /jet/lib/python3.6/site-packages (from requests->torchtext) (2.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchtext tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, sys\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from torchtext import data, datasets\n",
    "from torchtext.vocab import GloVe, FastText, CharNGram\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pwd\n",
    "#!tar xvzf nmr-100k.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = pd.read_csv('/jet/prs/workspace/nmr-100k.csv',delimiter='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECK\n",
    "print(len(tab.loc[0,'rviw_modd'].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab['label'] = np.where(tab.rviw_rate>7, \"pos\", np.where(tab.rviw_rate==1, \"neg\", \"neu\"))\n",
    "tab['input'] = tab.rviw_modd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>label</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_train</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>0.111</td>\n",
       "      <td>0.186</td>\n",
       "      <td>0.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>0.100</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "label       neg    neu    pos\n",
       "is_train                     \n",
       "False     0.111  0.186  0.703\n",
       "True      0.100  0.189  0.712"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(tab.is_train, tab.label, margins=False).apply(lambda r: round(r/r.sum(),3), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab[tab.is_train==True][tab.label.isin(['pos','neg'])].loc[:,['label','input']].to_json('train.json',orient='records',lines=True) #.sample(frac=0.1, replace=False)\n",
    "tab[tab.is_train!=True][tab.label.isin(['pos','neg'])].loc[:,['label','input']].to_json('valid.json',orient='records',lines=True) #.sample(frac=0.1, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = data.Field(fix_length=50, batch_first=True)\n",
    "LABEL = data.Field(sequential=False,)\n",
    "\n",
    "fields = {'label': ('label', LABEL), 'input': ('input', INPUT)}\n",
    "\n",
    "train, valid = data.TabularDataset.splits(\n",
    "    path = '/jet/prs/workspace',\n",
    "    train = 'train.json',\n",
    "    test = 'valid.json',\n",
    "    format = 'json',\n",
    "    fields = fields\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'pos', 'input': ['24151', '49437', '14146', '29768', '16450', '12248', '12248', '24151', '49437', '44879', '18715', '51458', '43907', '20743', '24116', '27813', '18196', '48998', '43703', '34396', '6247', '40398', '30707', '24173', '7767', '35830', '43062', '56983', '6322', '48998', '21323', '14146', '52824', '10', '23685', '52516', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']}\n"
     ]
    }
   ],
   "source": [
    "print(vars(train[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT.build_vocab(train, vectors=GloVe(name='6B', dim=300), max_size=10000, min_freq=10)\n",
    "LABEL.build_vocab(train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
      "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
     ]
    }
   ],
   "source": [
    "g_train, g_valid = data.BucketIterator.splits((train, valid), batch_size=32, device=-1, shuffle=True, sort=False)\n",
    "g_train.repeat = False\n",
    "g_valid.repeat = False\n",
    "dataloader = {'train':g_train, 'valid':g_valid}\n",
    "dataset_sizes = {'train':len(g_train.dataset),'valid':len(g_valid.dataset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50])\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECK\n",
    "batch = next(iter(dataloader['train']))\n",
    "x_input = batch.input.cuda()\n",
    "y_label = batch.label.cuda()\n",
    "print(x_input.size())\n",
    "print(y_label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gph(nn.Module):\n",
    "        \n",
    "    def __init__(self, n_vocab, n_hidden, n_label, btch_size=1, kernel_size=3, max_len=50):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.btch_size = btch_size\n",
    "        self.embd = nn.Embedding(n_vocab, n_hidden)\n",
    "        self.conv1d = nn.Conv1d(max_len, n_hidden, kernel_size)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(10)\n",
    "        self.fcn = nn.Linear(1000, n_label)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x_input):\n",
    "        btch_size = x_input.size()[0]\n",
    "        if btch_size != self.btch_size:\n",
    "            self.btch_size = btch_size\n",
    "        embd = self.embd(x_input)\n",
    "        conv1d = self.conv1d(embd) \n",
    "        pool = self.pool(conv1d)\n",
    "        flat = pool.view(self.btch_size, -1)\n",
    "        fcn = F.dropout(self.fcn(flat), p=0.5)\n",
    "        return self.softmax(fcn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET PARMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vocab = len(INPUT.vocab)\n",
    "n_hidden = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gph = Gph(n_vocab, n_hidden, n_label=3, btch_size=32, kernel_size=2)\n",
    "if torch.cuda.is_available():\n",
    "    gph = gph.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = optim.Adam(gph.parameters(), lr=learning_rate)\n",
    "\n",
    "#gph.embd.weight.requires_grad = False\n",
    "#optimizer = optim.SGD([parm for parm in gph.parameters() if parm.requires_grad==True], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gph(gph, optimizer, dataloader, phase='train', volatile=False):\n",
    "    if phase == 'train':\n",
    "        gph.train()\n",
    "    if phase == 'valid':\n",
    "        gph.eval()\n",
    "        volatile = True\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    running_counts = 0\n",
    "\n",
    "    for idx, batch in enumerate(dataloader[phase]):\n",
    "        if torch.cuda.is_available():\n",
    "            x_input, y_label = batch.input.cuda(), batch.label.cuda()\n",
    "\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "        y_prob = gph(x_input)\n",
    "        _, y_pred = torch.max(y_prob, 1)\n",
    "        loss = F.nll_loss(y_prob, y_label)\n",
    "\n",
    "        if phase == 'train':\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_corrects += torch.sum(y_pred == y_label.data).item()\n",
    "        running_counts += len(y_pred)\n",
    "\n",
    "    epoch_loss = running_loss / running_counts\n",
    "    epoch_acc = running_corrects / running_counts\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 0.0053 Acc: 0.9365\n",
      "valid Loss: 0.0059 Acc: 0.9281\n",
      "train Loss: 0.0049 Acc: 0.9409\n",
      "valid Loss: 0.0060 Acc: 0.9280\n",
      "train Loss: 0.0049 Acc: 0.9417\n",
      "valid Loss: 0.0060 Acc: 0.9276\n",
      "train Loss: 0.0048 Acc: 0.9422\n",
      "valid Loss: 0.0060 Acc: 0.9278\n",
      "train Loss: 0.0048 Acc: 0.9428\n",
      "valid Loss: 0.0058 Acc: 0.9283\n",
      "train Loss: 0.0048 Acc: 0.9432\n",
      "valid Loss: 0.0060 Acc: 0.9281\n",
      "train Loss: 0.0047 Acc: 0.9437\n",
      "valid Loss: 0.0059 Acc: 0.9296\n",
      "train Loss: 0.0047 Acc: 0.9442\n",
      "valid Loss: 0.0059 Acc: 0.9300\n",
      "train Loss: 0.0046 Acc: 0.9448\n",
      "valid Loss: 0.0059 Acc: 0.9293\n",
      "train Loss: 0.0046 Acc: 0.9452\n",
      "valid Loss: 0.0061 Acc: 0.9292\n",
      "train Loss: 0.0046 Acc: 0.9457\n",
      "valid Loss: 0.0061 Acc: 0.9280\n",
      "train Loss: 0.0045 Acc: 0.9460\n",
      "valid Loss: 0.0061 Acc: 0.9287\n",
      "train Loss: 0.0045 Acc: 0.9465\n",
      "valid Loss: 0.0062 Acc: 0.9278\n",
      "train Loss: 0.0045 Acc: 0.9469\n",
      "valid Loss: 0.0062 Acc: 0.9278\n",
      "train Loss: 0.0045 Acc: 0.9471\n",
      "valid Loss: 0.0063 Acc: 0.9285\n"
     ]
    }
   ],
   "source": [
    "# DEBUGGED\n",
    "# https://discuss.pytorch.org/t/data-iterator-failing-on-dev-set-only/11956/4\n",
    "train_loss, train_acc = [],[]\n",
    "valid_loss, valid_acc = [],[]\n",
    "for epoch in range(1,16):\n",
    "    epoch_loss, epoch_acc = train_gph(gph, optimizer, dataloader, phase='train')\n",
    "    vld_loss, vld_acc = train_gph(gph, optimizer, dataloader, phase='valid')\n",
    "    train_loss.append(epoch_loss)\n",
    "    train_acc.append(epoch_acc)\n",
    "    valid_loss.append(vld_loss)\n",
    "    valid_acc.append(vld_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
